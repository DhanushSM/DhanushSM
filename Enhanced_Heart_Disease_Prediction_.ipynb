{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtFdKV0OBXfFutqKvbiWcW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhanushSM/DhanushSM/blob/main/Enhanced_Heart_Disease_Prediction_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpOb3-TESx9J",
        "outputId": "989b5011-8264-4439-b3c8-9e001f52fab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         ENHANCED HEART DISEASE PREDICTION SYSTEM\n",
            "============================================================\n",
            "\n",
            "What would you like to do?\n",
            "  1: [TRAIN] Run enhanced training pipeline (needs heart.csv)\n",
            "  2: [PREDICT] Predict on a new patient (needs trained models)\n",
            "  3: [BOTH] Train then predict (complete workflow)\n",
            "\n",
            "Enter your choice (1, 2, or 3): 2\n",
            "--- 🩺 Heart Disease Prediction (PREDICT MODE) ---\n",
            "✅ Model and preprocessor loaded successfully.\n",
            "\n",
            "--- 🏥 Please enter new patient data ---\n",
            "Enter Age (e.g., 52): 55\n",
            "Enter Resting Blood Pressure (e.g., 120): 180\n",
            "Enter Cholesterol (e.g., 284): 250\n",
            "Enter Max Heart Rate (e.g., 162): 140\n",
            "Enter ST Depression (oldpeak, e.g., 1.5): 2\n",
            "Enter Sex (1 = male; 0 = female): 1\n",
            "Enter Chest Pain Type (0-3): 0\n",
            "Fasting Blood Sugar > 120 mg/dl (1 = true; 0 = false): 1\n",
            "Enter Resting ECG (0-2): 2\n",
            "Enter Exercise Induced Angina (1 = yes; 0 = no): 1\n",
            "Enter Slope of ST segment (0-2): 2\n",
            "Enter Vessels Colored by Flourosopy (0-4): 1\n",
            "Enter Thalassemia (0-3): 2\n",
            "-----------------------------------\n",
            "Data collected successfully.\n",
            "\n",
            "==================================================\n",
            "📈 PREDICTION RESULTS\n",
            "==================================================\n",
            "🟢 RESULT: LOW RISK OF HEART DISEASE\n",
            "📊 Confidence: 88.58%\n",
            "🎯 Probability of Heart Disease: 11.42%\n",
            "📋 Prediction Probabilities:\n",
            "   - No Disease: 88.58%\n",
            "   - Disease:    11.42%\n",
            "   ✅ Low risk - Maintain healthy lifestyle\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "\n",
        "# --- Scikit-learn Imports ---\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#################################################################\n",
        "# 1. ENHANCED DATA PROCESSOR CLASS\n",
        "#################################################################\n",
        "\n",
        "class EnhancedDataProcessor:\n",
        "    \"\"\"\n",
        "    Enhanced data processor with better validation and quality checks.\n",
        "    \"\"\"\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.data = None\n",
        "        self.preprocessor = None\n",
        "        # Original column names -> Friendly names\n",
        "        self.column_mapping = {\n",
        "            'cp': 'chest_pain_type',\n",
        "            'trestbps': 'resting_blood_pressure',\n",
        "            'chol': 'cholesterol',\n",
        "            'fbs': 'fasting_blood_sugar',\n",
        "            'restecg': 'resting_ecg',\n",
        "            'thalach': 'max_heart_rate',\n",
        "            'exang': 'exercise_angina',\n",
        "            'ca': 'vessels_colored',\n",
        "            'thal': 'thalassemia'\n",
        "        }\n",
        "\n",
        "    def load_data(self):\n",
        "        print(\"  Loading data...\")\n",
        "        try:\n",
        "            self.data = pd.read_csv(self.filepath)\n",
        "            print(colored(f\"  Data loaded successfully. Shape: {self.data.shape}\", \"green\"))\n",
        "        except FileNotFoundError:\n",
        "            print(colored(f\"Error: File not found at {self.filepath}\", \"red\"))\n",
        "            self.data = None\n",
        "        except Exception as e:\n",
        "            print(colored(f\"Error loading data: {e}\", \"red\"))\n",
        "            self.data = None\n",
        "\n",
        "    def validate_data_quality(self, target_variable=\"target\"):\n",
        "        \"\"\"Enhanced data validation with comprehensive checks\"\"\"\n",
        "        if self.data is None:\n",
        "            return False\n",
        "\n",
        "        print(\"\\n--- Data Quality Check ---\")\n",
        "\n",
        "        # Check for missing values\n",
        "        missing_values = self.data.isnull().sum()\n",
        "        if missing_values.sum() > 0:\n",
        "            print(colored(f\"  Missing values found:\", \"yellow\"))\n",
        "            for col, count in missing_values[missing_values > 0].items():\n",
        "                print(f\"    {col}: {count} missing values\")\n",
        "        else:\n",
        "            print(\"  No missing values detected ✓\")\n",
        "\n",
        "        # Check target variable distribution\n",
        "        if target_variable in self.data.columns:\n",
        "            target_dist = self.data[target_variable].value_counts()\n",
        "            print(f\"  Target distribution:\\n{target_dist}\")\n",
        "            if len(target_dist) == 2:\n",
        "                ratio = target_dist[1] / target_dist[0]\n",
        "                print(f\"  Class ratio: {ratio:.2f}:1\")\n",
        "\n",
        "                if target_dist.min() < len(self.data) * 0.3:\n",
        "                    print(colored(\"  ⚠️  Significant class imbalance detected!\", \"yellow\"))\n",
        "                else:\n",
        "                    print(\"  Balanced dataset ✓\")\n",
        "\n",
        "        # Check for outliers in numerical features\n",
        "        numerical_cols = ['age', 'resting_blood_pressure', 'cholesterol', 'max_heart_rate', 'oldpeak']\n",
        "        numerical_cols = [col for col in numerical_cols if col in self.data.columns]\n",
        "\n",
        "        print(\"  Outlier detection:\")\n",
        "        for col in numerical_cols:\n",
        "            Q1 = self.data[col].quantile(0.25)\n",
        "            Q3 = self.data[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            outliers = ((self.data[col] < (Q1 - 1.5 * IQR)) |\n",
        "                       (self.data[col] > (Q3 + 1.5 * IQR))).sum()\n",
        "            if outliers > 0:\n",
        "                print(f\"    {col}: {outliers} outliers detected\")\n",
        "            else:\n",
        "                print(f\"    {col}: No outliers ✓\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def rename_columns(self):\n",
        "        if self.data is not None:\n",
        "            self.data.rename(columns=self.column_mapping, inplace=True)\n",
        "            print(\"  Columns renamed for clarity.\")\n",
        "\n",
        "    def drop_duplicates(self):\n",
        "        if self.data is not None:\n",
        "            rows_dropped = len(self.data)\n",
        "            self.data.drop_duplicates(inplace=True)\n",
        "            rows_dropped -= len(self.data)\n",
        "            if rows_dropped > 0:\n",
        "                print(f\"  Dropped {rows_dropped} duplicate rows.\")\n",
        "            else:\n",
        "                print(\"  No duplicate rows found ✓\")\n",
        "\n",
        "    def handle_missing_values(self):\n",
        "        if self.data is not None:\n",
        "            rows_before = len(self.data)\n",
        "            self.data.dropna(inplace=True)\n",
        "            rows_after = len(self.data)\n",
        "            rows_dropped = rows_before - rows_after\n",
        "            if rows_dropped > 0:\n",
        "                print(f\"  Dropped {rows_dropped} rows with missing values.\")\n",
        "            else:\n",
        "                print(\"  No rows with missing values to drop ✓\")\n",
        "\n",
        "    def build_preprocessor(self, numerical_cols, categorical_cols):\n",
        "        \"\"\"\n",
        "        Builds a ColumnTransformer to handle all preprocessing.\n",
        "        \"\"\"\n",
        "        print(\"  Building data preprocessor...\")\n",
        "\n",
        "        # Numerical pipeline: Impute with median, then scale\n",
        "        numeric_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "\n",
        "        # Categorical pipeline: Impute with most frequent, then one-hot encode\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "        ])\n",
        "\n",
        "        # Combine transformers\n",
        "        self.preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numerical_cols),\n",
        "                ('cat', categorical_transformer, categorical_cols)\n",
        "            ],\n",
        "            remainder='passthrough'\n",
        "        )\n",
        "        print(colored(\"  Preprocessor built successfully.\", \"green\"))\n",
        "\n",
        "    def process_training_data(self, X):\n",
        "        \"\"\"\n",
        "        Fits the preprocessor to the training data and transforms it.\n",
        "        \"\"\"\n",
        "        if self.preprocessor is None:\n",
        "            print(colored(\"Error: Preprocessor not built yet.\", \"red\"))\n",
        "            return None\n",
        "\n",
        "        print(\"  Fitting and transforming training data...\")\n",
        "        X_processed = self.preprocessor.fit_transform(X)\n",
        "        feature_names = self.preprocessor.get_feature_names_out()\n",
        "        print(f\"  Training data processed. New shape: {X_processed.shape}\")\n",
        "        print(f\"  Number of features after preprocessing: {len(feature_names)}\")\n",
        "        return X_processed\n",
        "\n",
        "    def save_preprocessor(self, filepath=\"enhanced_data_preprocessor.pkl\"):\n",
        "        \"\"\"Saves the fitted preprocessor to a file.\"\"\"\n",
        "        if self.preprocessor:\n",
        "            joblib.dump(self.preprocessor, filepath)\n",
        "            print(colored(f\"  Data preprocessor saved to {filepath}\", \"cyan\"))\n",
        "        else:\n",
        "            print(colored(\"Error: No preprocessor to save.\", \"red\"))\n",
        "\n",
        "#################################################################\n",
        "# 2. ENHANCED MODEL TRAINER CLASS\n",
        "#################################################################\n",
        "\n",
        "class EnhancedModelTrainer:\n",
        "    \"\"\"\n",
        "    Enhanced model trainer with cross-validation and comprehensive evaluation.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "\n",
        "        # Enhanced models with tuned parameters\n",
        "        self.models = {\n",
        "            'Logistic Regression': LogisticRegression(\n",
        "                random_state=42, max_iter=2000, class_weight='balanced',\n",
        "                C=0.1, solver='liblinear'\n",
        "            ),\n",
        "            'Random Forest': RandomForestClassifier(\n",
        "                random_state=42, class_weight='balanced',\n",
        "                n_estimators=200, max_depth=10, min_samples_split=5\n",
        "            ),\n",
        "            'Support Vector Machine': SVC(\n",
        "                random_state=42, probability=True, class_weight='balanced',\n",
        "                C=1.0, kernel='rbf', gamma='scale'\n",
        "            ),\n",
        "            'Gradient Boosting': GradientBoostingClassifier(\n",
        "                random_state=42, n_estimators=100, max_depth=3,\n",
        "                learning_rate=0.1, subsample=0.8\n",
        "            )\n",
        "        }\n",
        "        self.results = {}\n",
        "        self.trained_models = {}\n",
        "        self.cv_results = {}\n",
        "\n",
        "    def perform_cross_validation(self, X, y, cv_folds=5):\n",
        "        \"\"\"Perform cross-validation for better model evaluation\"\"\"\n",
        "        print(f\"\\n--- Performing {cv_folds}-fold Cross-Validation ---\")\n",
        "        cv_results = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            try:\n",
        "                # Use accuracy for cross-validation\n",
        "                cv_scores = cross_val_score(model, X, y, cv=cv_folds, scoring='accuracy')\n",
        "                cv_results[name] = {\n",
        "                    'mean_accuracy': cv_scores.mean(),\n",
        "                    'std_accuracy': cv_scores.std(),\n",
        "                    'all_scores': cv_scores\n",
        "                }\n",
        "                print(colored(f\"  {name}: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\", \"yellow\"))\n",
        "            except Exception as e:\n",
        "                print(colored(f\"  {name}: CV failed - {e}\", \"red\"))\n",
        "\n",
        "        self.cv_results = cv_results\n",
        "        return cv_results\n",
        "\n",
        "    def split_data(self, X, y, test_size=0.2):\n",
        "        print(\"  Splitting data into train and test sets...\")\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=42, stratify=y\n",
        "        )\n",
        "        print(colored(f\"  Data split complete. Train: {self.X_train.shape[0]}, Test: {self.X_test.shape[0]}\", \"green\"))\n",
        "\n",
        "    def train_and_evaluate_all(self):\n",
        "        print(\"\\n--- Training & Evaluating Models ---\")\n",
        "        if self.X_train is None:\n",
        "            print(colored(\"Error: Data has not been split yet.\", \"red\"))\n",
        "            return\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"\\n  Training {name}...\")\n",
        "            try:\n",
        "                model.fit(self.X_train, self.y_train)\n",
        "                y_pred = model.predict(self.X_test)\n",
        "\n",
        "                # Comprehensive metrics\n",
        "                accuracy = accuracy_score(self.y_test, y_pred)\n",
        "                precision = precision_score(self.y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(self.y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(self.y_test, y_pred, zero_division=0)\n",
        "                report = classification_report(self.y_test, y_pred, zero_division=0)\n",
        "\n",
        "                self.results[name] = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1_score': f1,\n",
        "                    'report': report\n",
        "                }\n",
        "                self.trained_models[name] = model\n",
        "\n",
        "                print(colored(f\"  ✅ {name} Results:\", \"green\"))\n",
        "                print(f\"    Accuracy:  {accuracy * 100:.2f}%\")\n",
        "                print(f\"    Precision: {precision * 100:.2f}%\")\n",
        "                print(f\"    Recall:    {recall * 100:.2f}%\")\n",
        "                print(f\"    F1-Score:  {f1 * 100:.2f}%\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(colored(f\"  ❌ {name} training failed: {e}\", \"red\"))\n",
        "\n",
        "    def plot_all_confusion_matrices(self):\n",
        "        print(\"\\n--- Generating Confusion Matrices ---\")\n",
        "        num_models = len(self.trained_models)\n",
        "        if num_models == 0:\n",
        "            print(\"No trained models available for plotting.\")\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "        axes = axes.flat\n",
        "\n",
        "        for i, (name, model) in enumerate(self.trained_models.items()):\n",
        "            if i < len(axes):\n",
        "                ax = axes[i]\n",
        "                ConfusionMatrixDisplay.from_estimator(\n",
        "                    model, self.X_test, self.y_test,\n",
        "                    ax=ax, cmap=plt.cm.Blues, colorbar=False\n",
        "                )\n",
        "                ax.set_title(f'{name}\\nAccuracy: {self.results[name][\"accuracy\"]*100:.1f}%')\n",
        "\n",
        "        # Hide empty subplots\n",
        "        for i in range(len(self.trained_models), len(axes)):\n",
        "            axes[i].set_visible(False)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show(block=False)\n",
        "        print(\"  Confusion matrices displayed.\")\n",
        "\n",
        "    def plot_cv_comparison(self):\n",
        "        \"\"\"Plot cross-validation results comparison\"\"\"\n",
        "        if not self.cv_results:\n",
        "            print(\"No cross-validation results to plot.\")\n",
        "            return\n",
        "\n",
        "        names = list(self.cv_results.keys())\n",
        "        means = [self.cv_results[name]['mean_accuracy'] for name in names]\n",
        "        stds = [self.cv_results[name]['std_accuracy'] for name in names]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        y_pos = np.arange(len(names))\n",
        "\n",
        "        plt.barh(y_pos, means, xerr=stds, align='center', alpha=0.7, capsize=5)\n",
        "        plt.yticks(y_pos, names)\n",
        "        plt.xlabel('Accuracy')\n",
        "        plt.title('Cross-Validation Results (Mean ± Std Dev)')\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for i, v in enumerate(means):\n",
        "            plt.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show(block=False)\n",
        "\n",
        "    def get_best_model(self):\n",
        "        \"\"\"Finds the best model based on weighted score (accuracy + f1)\"\"\"\n",
        "        if not self.results:\n",
        "            return None, 0, None\n",
        "\n",
        "        # Use weighted score considering both accuracy and f1-score\n",
        "        def calculate_score(result):\n",
        "            return (result['accuracy'] + result['f1_score']) / 2\n",
        "\n",
        "        best_name = max(self.results, key=lambda name: calculate_score(self.results[name]))\n",
        "        best_score = self.results[best_name]['accuracy']\n",
        "        best_model_object = self.trained_models[best_name]\n",
        "\n",
        "        return best_name, best_score, best_model_object\n",
        "\n",
        "    def save_model(self, model, filepath=\"enhanced_heart_model.pkl\"):\n",
        "        \"\"\"Saves the best performing model to a file.\"\"\"\n",
        "        joblib.dump(model, filepath)\n",
        "        print(colored(f\"  Model saved to {filepath}\", \"cyan\"))\n",
        "\n",
        "#################################################################\n",
        "# 3. ENHANCED TRAINING PIPELINE\n",
        "#################################################################\n",
        "\n",
        "def print_feature_importance(model, preprocessor, original_columns):\n",
        "    \"\"\"Enhanced feature importance analysis\"\"\"\n",
        "    print(colored(\"\\n--- Feature Importance Analysis ---\", \"yellow\"))\n",
        "\n",
        "    try:\n",
        "        feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "        if hasattr(model, 'feature_importances_'):  # Tree-based models\n",
        "            importances = model.feature_importances_\n",
        "            importance_type = \"Feature Importance\"\n",
        "        elif hasattr(model, 'coef_'):  # Linear models\n",
        "            importances = np.abs(model.coef_[0])\n",
        "            importance_type = \"Absolute Coefficient\"\n",
        "        else:\n",
        "            print(\"Feature importance not available for this model type.\")\n",
        "            return\n",
        "\n",
        "        # Create feature importance dataframe\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            importance_type: importances\n",
        "        }).sort_values(importance_type, ascending=False)\n",
        "\n",
        "        print(f\"Top 10 Most Important Features ({importance_type}):\")\n",
        "        print(importance_df.head(10).to_string(index=False))\n",
        "\n",
        "        # Plot feature importance\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        top_features = importance_df.head(10)\n",
        "        plt.barh(top_features['Feature'], top_features[importance_type])\n",
        "        plt.xlabel(importance_type)\n",
        "        plt.title(f'Top 10 Feature Importances\\n({importance_type})')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.show(block=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not compute feature importance: {e}\")\n",
        "\n",
        "def print_final_evaluation(model_name, accuracy, results):\n",
        "    \"\"\"Print comprehensive final evaluation\"\"\"\n",
        "    print(colored(\"\\n\" + \"=\"*60, \"green\"))\n",
        "    print(colored(\"🎯 FINAL TRAINING RESULTS\", \"green\", attrs=['bold']))\n",
        "    print(colored(\"=\"*60, \"green\"))\n",
        "    print(f\"🏆 Best Model: {model_name}\")\n",
        "    print(f\"📊 Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"📈 Precision: {results['precision'] * 100:.2f}%\")\n",
        "    print(f\"📈 Recall: {results['recall'] * 100:.2f}%\")\n",
        "    print(f\"📈 F1-Score: {results['f1_score'] * 100:.2f}%\")\n",
        "    print(f\"\\n📋 Detailed Classification Report:\")\n",
        "    print(results['report'])\n",
        "\n",
        "def run_enhanced_training_pipeline():\n",
        "    \"\"\"\n",
        "    Enhanced training pipeline with comprehensive validation and evaluation\n",
        "    \"\"\"\n",
        "    print(colored(\"--- 🚀 ENHANCED Heart Disease Prediction Pipeline (TRAIN MODE) ---\", \"cyan\", attrs=['bold']))\n",
        "\n",
        "    # Configuration\n",
        "    DATA_FILEPATH = \"heart.csv\"\n",
        "    TARGET_VARIABLE = \"target\"\n",
        "\n",
        "    NUMERICAL_COLS_TO_SCALE = [\n",
        "        'age', 'resting_blood_pressure', 'cholesterol',\n",
        "        'max_heart_rate', 'oldpeak'\n",
        "    ]\n",
        "    CATEGORICAL_COLS_TO_ENCODE = [\n",
        "        'chest_pain_type', 'resting_ecg', 'slope',\n",
        "        'vessels_colored', 'thalassemia',\n",
        "        'fasting_blood_sugar', 'exercise_angina', 'sex'\n",
        "    ]\n",
        "\n",
        "    # --- 1. Enhanced Preprocessing ---\n",
        "    print(\"\\n--- 1. Enhanced Data Preprocessing ---\")\n",
        "    processor = EnhancedDataProcessor(DATA_FILEPATH)\n",
        "    processor.load_data()\n",
        "    if processor.data is None:\n",
        "        return None, None, None\n",
        "\n",
        "    processor.rename_columns()\n",
        "    processor.drop_duplicates()\n",
        "    processor.handle_missing_values()\n",
        "\n",
        "    # Enhanced data validation\n",
        "    processor.validate_data_quality(TARGET_VARIABLE)\n",
        "\n",
        "    print(\"\\n--- Data Summary ---\")\n",
        "    print(f\"Dataset shape: {processor.data.shape}\")\n",
        "    print(f\"Features: {list(processor.data.columns)}\")\n",
        "\n",
        "    X = processor.data.drop(TARGET_VARIABLE, axis=1)\n",
        "    y = processor.data[TARGET_VARIABLE]\n",
        "\n",
        "    processor.build_preprocessor(NUMERICAL_COLS_TO_SCALE, CATEGORICAL_COLS_TO_ENCODE)\n",
        "    X_processed = processor.process_training_data(X)\n",
        "\n",
        "    if X_processed is None:\n",
        "        return None, None, None\n",
        "\n",
        "    # --- 2. Enhanced Model Training ---\n",
        "    print(\"\\n--- 2. Enhanced Model Training ---\")\n",
        "    trainer = EnhancedModelTrainer()\n",
        "\n",
        "    # Perform cross-validation first\n",
        "    cv_results = trainer.perform_cross_validation(X_processed, y, cv_folds=5)\n",
        "\n",
        "    # Plot CV results\n",
        "    trainer.plot_cv_comparison()\n",
        "\n",
        "    # Then proceed with standard train/test split\n",
        "    trainer.split_data(X_processed, y)\n",
        "    trainer.train_and_evaluate_all()\n",
        "\n",
        "    # --- 3. Enhanced Evaluation ---\n",
        "    print(\"\\n--- 3. Comprehensive Model Evaluation ---\")\n",
        "    trainer.plot_all_confusion_matrices()\n",
        "\n",
        "    best_name, best_score, best_model = trainer.get_best_model()\n",
        "\n",
        "    if best_model:\n",
        "        # Save artifacts\n",
        "        processor.save_preprocessor(\"enhanced_data_preprocessor.pkl\")\n",
        "        trainer.save_model(best_model, \"enhanced_heart_model.pkl\")\n",
        "\n",
        "        # Feature importance analysis\n",
        "        print_feature_importance(best_model, processor.preprocessor, X.columns)\n",
        "\n",
        "        # Final evaluation\n",
        "        print_final_evaluation(best_name, best_score, trainer.results[best_name])\n",
        "\n",
        "        return best_model, processor, trainer\n",
        "    else:\n",
        "        print(colored(\"Training failed - no model was successfully trained!\", \"red\"))\n",
        "        return None, None, None\n",
        "\n",
        "#################################################################\n",
        "# 4. PREDICTION APP (PREDICT MODE)\n",
        "#################################################################\n",
        "\n",
        "def get_patient_data_from_user():\n",
        "    \"\"\"\n",
        "    Prompts the user to enter data for a new patient.\n",
        "    Uses the *renamed, friendly* column names.\n",
        "    \"\"\"\n",
        "    print(colored(\"\\n--- 🏥 Please enter new patient data ---\", \"yellow\"))\n",
        "\n",
        "    patient_data = {}\n",
        "\n",
        "    try:\n",
        "        # --- Numerical Features ---\n",
        "        patient_data['age'] = int(input(\"Enter Age (e.g., 52): \"))\n",
        "        patient_data['resting_blood_pressure'] = int(input(\"Enter Resting Blood Pressure (e.g., 120): \"))\n",
        "        patient_data['cholesterol'] = int(input(\"Enter Cholesterol (e.g., 284): \"))\n",
        "        patient_data['max_heart_rate'] = int(input(\"Enter Max Heart Rate (e.g., 162): \"))\n",
        "        patient_data['oldpeak'] = float(input(\"Enter ST Depression (oldpeak, e.g., 1.5): \"))\n",
        "\n",
        "        # --- Categorical Features ---\n",
        "        patient_data['sex'] = int(input(\"Enter Sex (1 = male; 0 = female): \"))\n",
        "        patient_data['chest_pain_type'] = int(input(\"Enter Chest Pain Type (0-3): \"))\n",
        "        patient_data['fasting_blood_sugar'] = int(input(\"Fasting Blood Sugar > 120 mg/dl (1 = true; 0 = false): \"))\n",
        "        patient_data['resting_ecg'] = int(input(\"Enter Resting ECG (0-2): \"))\n",
        "        patient_data['exercise_angina'] = int(input(\"Enter Exercise Induced Angina (1 = yes; 0 = no): \"))\n",
        "        patient_data['slope'] = int(input(\"Enter Slope of ST segment (0-2): \"))\n",
        "        patient_data['vessels_colored'] = int(input(\"Enter Vessels Colored by Flourosopy (0-4): \"))\n",
        "        patient_data['thalassemia'] = int(input(\"Enter Thalassemia (0-3): \"))\n",
        "\n",
        "        print(colored(\"-----------------------------------\", \"yellow\"))\n",
        "        print(colored(\"Data collected successfully.\", \"green\"))\n",
        "        return patient_data\n",
        "\n",
        "    except ValueError:\n",
        "        print(colored(\"\\n[Error] Invalid input. Please enter numerical values only.\", \"red\"))\n",
        "        return None\n",
        "\n",
        "def run_prediction_app():\n",
        "    \"\"\"\n",
        "    Loads the saved model and preprocessor, then predicts on new user data.\n",
        "    \"\"\"\n",
        "    print(colored(\"--- 🩺 Heart Disease Prediction (PREDICT MODE) ---\", \"cyan\", attrs=['bold']))\n",
        "\n",
        "    # --- 1. Load Model and Preprocessor ---\n",
        "    try:\n",
        "        model = joblib.load(\"enhanced_heart_model.pkl\")\n",
        "        preprocessor = joblib.load(\"enhanced_data_preprocessor.pkl\")\n",
        "        print(colored(\"✅ Model and preprocessor loaded successfully.\", \"green\"))\n",
        "    except FileNotFoundError:\n",
        "        print(colored(\"❌ Error: Model files not found.\", \"red\"))\n",
        "        print(colored(\"Please run the training pipeline first to train and save the model.\", \"red\"))\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(colored(f\"An error occurred loading files: {e}\", \"red\"))\n",
        "        return\n",
        "\n",
        "    # --- 2. Get Data from User ---\n",
        "    patient_data_dict = get_patient_data_from_user()\n",
        "\n",
        "    if patient_data_dict is None:\n",
        "        print(colored(\"Prediction cancelled due to invalid input.\", \"red\"))\n",
        "        return\n",
        "\n",
        "    # --- 3. Process and Predict ---\n",
        "    try:\n",
        "        # Convert dictionary to DataFrame\n",
        "        patient_df = pd.DataFrame([patient_data_dict])\n",
        "\n",
        "        # Apply the preprocessor\n",
        "        processed_patient_data = preprocessor.transform(patient_df)\n",
        "\n",
        "        # Make the prediction\n",
        "        prediction = model.predict(processed_patient_data)\n",
        "        prediction_proba = model.predict_proba(processed_patient_data)\n",
        "\n",
        "        # --- 4. Show Results ---\n",
        "        confidence = prediction_proba[0][prediction[0]] * 100\n",
        "        disease_prob = prediction_proba[0][1] * 100  # Probability of disease\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(colored(\"📈 PREDICTION RESULTS\", \"cyan\", attrs=['bold']))\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        if prediction[0] == 1:\n",
        "            print(colored(f\"🔴 RESULT: HIGH RISK OF HEART DISEASE\", \"red\", attrs=['bold']))\n",
        "        else:\n",
        "            print(colored(f\"🟢 RESULT: LOW RISK OF HEART DISEASE\", \"green\", attrs=['bold']))\n",
        "\n",
        "        print(f\"📊 Confidence: {confidence:.2f}%\")\n",
        "        print(f\"🎯 Probability of Heart Disease: {disease_prob:.2f}%\")\n",
        "        print(f\"📋 Prediction Probabilities:\")\n",
        "        print(f\"   - No Disease: {prediction_proba[0][0]*100:.2f}%\")\n",
        "        print(f\"   - Disease:    {prediction_proba[0][1]*100:.2f}%\")\n",
        "\n",
        "        # Risk interpretation\n",
        "        if disease_prob > 70:\n",
        "            print(colored(\"   ⚠️  High risk - Consult a doctor immediately!\", \"red\"))\n",
        "        elif disease_prob > 30:\n",
        "            print(colored(\"   ℹ️  Moderate risk - Consider medical consultation\", \"yellow\"))\n",
        "        else:\n",
        "            print(colored(\"   ✅ Low risk - Maintain healthy lifestyle\", \"green\"))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(colored(f\"\\n[Error] Failed to make prediction: {e}\", \"red\"))\n",
        "        print(\"This may be due to a mismatch in data format or corrupted files.\")\n",
        "\n",
        "#################################################################\n",
        "# 5. MAIN EXECUTION (User Choice)\n",
        "#################################################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(colored(\"=\"*60, \"blue\"))\n",
        "    print(colored(\"         ENHANCED HEART DISEASE PREDICTION SYSTEM\", \"blue\", attrs=['bold']))\n",
        "    print(colored(\"=\"*60, \"blue\"))\n",
        "    print(\"\\nWhat would you like to do?\")\n",
        "    print(colored(\"  1: [TRAIN] \", \"green\") + \"Run enhanced training pipeline (needs heart.csv)\")\n",
        "    print(colored(\"  2: [PREDICT] \", \"yellow\") + \"Predict on a new patient (needs trained models)\")\n",
        "    print(colored(\"  3: [BOTH] \", \"cyan\") + \"Train then predict (complete workflow)\")\n",
        "\n",
        "    choice = input(\"\\nEnter your choice (1, 2, or 3): \")\n",
        "\n",
        "    if choice == '1':\n",
        "        best_model, processor, trainer = run_enhanced_training_pipeline()\n",
        "        if best_model:\n",
        "            print(colored(\"\\n🎉 Training completed successfully! You can now use prediction mode.\", \"green\"))\n",
        "\n",
        "        # Keep plots open\n",
        "        print(\"\\nPress Enter to close plots and exit...\")\n",
        "        input()\n",
        "\n",
        "    elif choice == '2':\n",
        "        run_prediction_app()\n",
        "\n",
        "    elif choice == '3':\n",
        "        # Train first\n",
        "        best_model, processor, trainer = run_enhanced_training_pipeline()\n",
        "        if best_model:\n",
        "            print(colored(\"\\n\" + \"=\"*50, \"green\"))\n",
        "            print(colored(\"🚀 NOW SWITCHING TO PREDICTION MODE\", \"green\", attrs=['bold']))\n",
        "            print(colored(\"=\"*50, \"green\"))\n",
        "            # Then predict\n",
        "            run_prediction_app()\n",
        "        else:\n",
        "            print(colored(\"Cannot proceed to prediction - training failed.\", \"red\"))\n",
        "\n",
        "    else:\n",
        "        print(colored(\"Invalid choice. Please run the script again and enter 1, 2, or 3.\", \"red\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1SWluxhaVNyb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}